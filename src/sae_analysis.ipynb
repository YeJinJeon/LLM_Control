{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from sae_anlaysis_utils import *\n",
    "\n",
    "model_path = \"google/gemma-2-2b\"\n",
    "save_model_path = save_model_path = \"/home/yejeon/models/\" + model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gemma-2-2b\"\n",
    "sae_id = \"20-gemmascope-res-16k\"\n",
    "f_idx = [16213, 12054, 2575, 129, 5856, 136, 14549, 9766, 482, 1902, 7357, 14117, 7507, 2358, 7214, 13001, 14281, 7921, 11871, 12025]\n",
    "\n",
    "feature_json = get_feature_details_from_neuronpedia(model_id, sae_id, \"16213\")\n",
    "top5_act_sentences = get_activation_senteces(feature_json[\"activations\"])\n",
    "top5_refined = [s+\"\\n\" for s in top5_act_sentences]\n",
    "\n",
    "# Print key info\n",
    "print(\"Feature Info:\")\n",
    "print(f\"  Top Positive Strings: {feature_json[\"pos_str\"]}\")\n",
    "print(f\"  Top Negative Strings: {feature_json[\"neg_str\"]}\")\n",
    "print(f\"  Top Activations:\")\n",
    "for i, s in enumerate(top5_act_sentences):        \n",
    "    print(f\"  \\t[{i}] {s}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
