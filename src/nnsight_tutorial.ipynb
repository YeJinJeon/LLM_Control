{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yejeon/envs/circuit/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import nnsight\n",
    "from nnsight import NNsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_dims = 10\n",
    "output_size = 2\n",
    "\n",
    "# define PyTorch model\n",
    "net = torch.nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"layer1\", torch.nn.Linear(input_size, hidden_dims)),\n",
    "            (\"layer2\", torch.nn.Linear(hidden_dims, output_size)),\n",
    "        ]\n",
    "    )\n",
    ").requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraps around a given PyTorch model to enable investigation of its internal parameters.\n",
    "# This added a couple properties to each module in the model (including the root model itself). The two most important ones are .input and .output.\n",
    "tiny_model = NNsight(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the context, how we can save activations and execute functions on the proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9780, 0.3155, 0.1368, 0.3514, 0.9709]])\n",
      "tensor([[-0.0975,  0.1905]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# with keyword to enter a context-like object. \n",
    "    - This object defines logic to be run at the start of the with block, as well as logic to be run when exiting.\n",
    "\n",
    "# being within the context - we can read from the file\n",
    "with open('myfile.txt', 'r') as file:\n",
    "  text = file.read()\n",
    "\"\"\"\n",
    "# nnsight uses contexts to enable intuitive access into the internals of a neural network. \n",
    "# Inside the context, we will be able to customize how the neural network runs. The model is actually run upon exiting the tracing context.\n",
    "\n",
    "# random input\n",
    "input = torch.rand((1, input_size))\n",
    "\n",
    "with tiny_model.trace(input) as tracer:\n",
    "\n",
    "    # Proxies for the eventual inputs and outputs of a module.\n",
    "    # Proxy objects will only have their value at the end of a context if we call .save() on them\n",
    "    input = tiny_model.layer1.input.save()\n",
    "    output = tiny_model.output.save() \n",
    "\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "# nnsight handles Pytorch functions and methods within the tracing context, by creating a Proxy request for it\n",
    "with tiny_model.trace(input):\n",
    "\n",
    "    # Note we don't need to call .save() on the output,\n",
    "    # as we're only using its value within the tracing context.\n",
    "    l1_output = tiny_model.layer1.output\n",
    "\n",
    "    # We do need to save the argmax tensor however,\n",
    "    # as we're using it outside the tracing context.\n",
    "    l1_amax = torch.argmax(l1_output, dim=1).save()\n",
    "\n",
    "print(l1_amax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything within the tracing context operates on the intervention graph. \n",
    "# Therefore, for nnsight to trace a function it must also be a part of the intervention graph.\n",
    "# How do we add them to the intervention graph? Enter nnsight.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracer vs Invoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we call .trace(...), it’s actually creating two different contexts behind the scenes. \n",
    "# The first one is the tracing context that we’ve discussed previously,\n",
    "# and the second one is the invoker context\n",
    "# The invoker context defines the values of the .input and .output\n",
    "\n",
    "# If we call .trace() without an input, then we can call tracer.invoke(input1) to manually create the invoker context with an input\n",
    "# interventions within its context will only refer to the input in that particular invoke statement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
